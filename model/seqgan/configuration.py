
max_seq_len = 46
start_token = 17
class generator_config(object):
    """Wrapper class for generator hyperparameter"""

    def __init__(self):
        self.emb_dim = 32  # dimension of embedding
        self.num_emb = 18  # dimension of output unit
        self.hidden_dim = 32  # dimension of hidden unit
        self.sequence_length = max_seq_len  # maximum input sequence length
        self.gen_batch_size = 64  # batch size of generator
        self.start_token = start_token  # special token for start of sentence


class discriminator_config(object):
    """Wrapper class for discriminator hyperparameter"""

    def __init__(self):
        self.sequence_length = max_seq_len  # maximum input sequence length
        self.num_classes = 2  # number of class (real and fake)
        self.vocab_size = 18  # vocabulary size, shoud be same as num_emb
        self.dis_embedding_dim = 64  # dimension of discriminator embedding space
        self.dis_filter_sizes = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 15, 20]  # convolutional kernel size of discriminator
        self.dis_num_filters = [100, 200, 200, 200, 200, 100, 100, 100, 100, 100, 160,
                                160]  # number of filters of each conv. kernel
        self.dis_dropout_keep_prob = 0.75  # dropout rate of discriminator
        self.dis_l2_reg_lambda = 0.2  # L2 regularization strength
        self.dis_batch_size = 64  # Batch size for discriminator
        self.dis_learning_rate = 1e-4  # Learning rate of discriminator


class training_config(object):
    """Wrapper class for parameters for training"""
    def __init__(self):
        self.gen_learning_rate = 0.01  # learning rate of generator
        self.gen_update_time = 1  # update times of generator in adversarial training
        self.dis_update_time_adv = 5  # update times of discriminator in adversarial training
        self.dis_update_epoch_adv = 3  # update epoch / times of discriminator
        self.dis_update_time_pre = 50  # pretraining times of discriminator
        self.dis_update_epoch_pre = 3  # number of epoch / time in pretraining
        self.pretrained_epoch_num = 20  # Number of pretraining epoch
        self.rollout_num = 16  # Rollout number for reward estimation
        self.test_per_epoch = 10  # Test the NLL per epoch
        self.batch_size = 64  # Batch size used for training
        self.save_pretrained = 120  # Whether to save model in certain epoch (optional)
        self.grad_clip = 5.0  # Gradient Clipping
        self.seed = 88  # Random seed used for initialization
        self.start_token = start_token  # special start token
        self.total_batch = 100  # total batch used for adversarial training
        self.positive_file = "data/modbus/modbus_raw_data_46_6w.txt"  # save path of real data generated by target LSTM
        self.negative_file = "save/generator_sample.txt"  # save path of fake data generated by generator
        self.eval_file = "save/eval_file.txt"  # file used for evaluation
        self.generated_num = 60000  # Number of samples from generator used for evaluation
        self.sequence_length = max_seq_len  # maximum input sequence length
        self.output_dir = r'output/modbus'
        self.save_generated_num = 10240
        self.model_name = r"GanBased_SEQ"

